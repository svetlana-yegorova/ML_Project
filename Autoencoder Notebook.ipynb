{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice using an autoencoder on an MNIST dataset\n",
    "Dowload the MNIST dataset\n",
    "Specify the neural network\n",
    "Test reconstruction accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset: \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets # Import the sklearn datasets module\n",
    "from sklearn.datasets import fetch_openml # add this line to get th mnist dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "\n",
    "# 70,000 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size: (70000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Dataset Size:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training (52500, 784)\n",
      "testing (17500, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into testing and training parts: \n",
    "X_train,X_test,y_tr,y_tst = train_test_split(X,y)\n",
    "print(\"training\", X_train.shape)\n",
    "print(\"testing\", X_test.shape)\n",
    "type(y_tr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tensor package\n",
    "convert dataset to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X_train = torch.from_numpy(np.asarray(X_train))\n",
    "X_test = torch.from_numpy(np.asarray(X_test))\n",
    "y_train = y_tr.astype(np.float)\n",
    "y_test=y_tst.astype(np.float)\n",
    "y_train = torch.from_numpy(np.asarray(y_train)) \n",
    "\n",
    "y_test = torch.from_numpy(np.asarray(y_test))\n",
    "\n",
    "\n",
    "X_train = X_train.to(torch.float32) # \n",
    "X_test = X_test.to(torch.float32)\n",
    "y_train = y_train.to(torch.long)\n",
    "y_test = y_test.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the network structure **kwargs keeps the input general \n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=kwargs[\"input_shape\"], out_features=128\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_hidden_layer = nn.Linear(\n",
    "            in_features=128, out_features=128\n",
    "        )\n",
    "        self.decoder_output_layer = nn.Linear(\n",
    "            in_features=128, out_features=kwargs[\"input_shape\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        code = torch.relu(code)                      # so, this is the latent space, consisting of 128 features? \n",
    "        activation = self.decoder_hidden_layer(code)\n",
    "        activation = torch.relu(activation)\n",
    "        activation = self.decoder_output_layer(activation)\n",
    "        reconstructed = torch.relu(activation)\n",
    "        return reconstructed\n",
    "# so what is the \"latent variable here? \" - I think \"code\" represents the latent space/varaibles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_shape=784).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data: \n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# train_dataset = torchvision.datasets.MNIST(\n",
    "#     root=\"~/torch_datasets\", train=True, transform=transform, download=True)\n",
    "\n",
    "# test_dataset = torchvision.datasets.MNIST(\n",
    "#     root=\"~/torch_datasets\", train=False, transform=transform, download=True)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     X_train, batch_size=128, shuffle=True, num_workers=4, pin_memory=True) # what does num_workers do? \n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X_train,y_train)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/40, loss = 2763.270143\n",
      "epoch : 2/40, loss = 1327.143798\n",
      "epoch : 3/40, loss = 1113.643034\n",
      "epoch : 4/40, loss = 1011.562628\n",
      "epoch : 5/40, loss = 950.037427\n",
      "epoch : 6/40, loss = 908.574543\n",
      "epoch : 7/40, loss = 875.041446\n",
      "epoch : 8/40, loss = 850.229914\n",
      "epoch : 9/40, loss = 831.515134\n",
      "epoch : 10/40, loss = 814.384805\n",
      "epoch : 11/40, loss = 801.481276\n",
      "epoch : 12/40, loss = 789.629551\n",
      "epoch : 13/40, loss = 779.425451\n",
      "epoch : 14/40, loss = 769.297536\n",
      "epoch : 15/40, loss = 761.237524\n",
      "epoch : 16/40, loss = 753.788312\n",
      "epoch : 17/40, loss = 745.497186\n",
      "epoch : 18/40, loss = 739.219666\n",
      "epoch : 19/40, loss = 731.695678\n",
      "epoch : 20/40, loss = 727.542496\n",
      "epoch : 21/40, loss = 720.688714\n",
      "epoch : 22/40, loss = 715.613923\n",
      "epoch : 23/40, loss = 711.234643\n",
      "epoch : 24/40, loss = 705.824387\n",
      "epoch : 25/40, loss = 701.862906\n",
      "epoch : 26/40, loss = 697.881286\n",
      "epoch : 27/40, loss = 694.419936\n",
      "epoch : 28/40, loss = 691.345997\n",
      "epoch : 29/40, loss = 686.548113\n",
      "epoch : 30/40, loss = 685.919163\n",
      "epoch : 31/40, loss = 681.971588\n",
      "epoch : 32/40, loss = 679.626392\n",
      "epoch : 33/40, loss = 677.528631\n",
      "epoch : 34/40, loss = 675.643855\n",
      "epoch : 35/40, loss = 672.426620\n",
      "epoch : 36/40, loss = 671.864730\n",
      "epoch : 37/40, loss = 670.019750\n",
      "epoch : 38/40, loss = 667.974737\n",
      "epoch : 39/40, loss = 666.125117\n",
      "epoch : 40/40, loss = 664.823714\n"
     ]
    }
   ],
   "source": [
    "epochs=40\n",
    "loss=0\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.view(-1, 784).to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n"
     ]
    }
   ],
   "source": [
    "# how do we get to the weights? and the outputs \n",
    "print(outputs.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
